# Multi Agent System

## PROMPT

This screening tool takes in a RIS file and inclusion/exclusion criteria from the user. The RIS file is parsed and uploaded into Supabase where each study is a row and columns are the metadata (title, abstract, keywords, etc.) and also a decision and rationale column. AI agents then query the database, check whether the “decision” column is empty. If empty, the study title, abstract, keywords are pulled from the database, and the AI agent makes a decision on whether to include/exclude/maybe the study. Once finished, the user is presented with 4 buttons to download the three RIS files (included, maybe, excluded) and an Excel file explaining the reasons for each inclusion/exclusion/maybe.

**Stack**: Next.js (TypeScript), FastAPI (Python), Supabase  
**Packages**: rispy, OpenAI, CrewAI, litellm  
**AI Integration**: OpenAI’s GPT-4o-mini, CrewAI

The frontend is already built on Next.js, where a user uploads a RIS file and also their inclusion/exclusion criteria to the backend.

---

## DATA FLOW

### Frontend

1. User uploads the RIS file along with the user’s inclusion/exclusion criteria (inex criteria).  
2. *(Optional validation)* The user’s inex criteria are first checked to make sure they are not ambiguous (inex criteria text is sent to an LLM model and asked whether the inex criteria are clear).  
3. The LLM model either confirms or rejects the user’s input.  
   - If successful, proceed to sending the RIS file and inex criteria to the backend.  
   - If not, prompt the user to try again.  
4. RIS file + inex criteria are sent to the backend for processing.

### Backend

1. **RIS File Parsing**  
   - The RIS file is parsed into individual studies using `rispy`.  
   - Individual studies are further parsed into their respective metadata, e.g., title, abstract, keywords, page numbers, journal, etc.

2. **Supabase Database Storage**  
   - Each study is placed into a row in Supabase.  
   - Each column corresponds to a different piece of metadata (e.g., title, abstract, keywords, etc.) and includes additional columns for `decision` and `decision rationale`.  
   - The original file is also stored in Supabase.

3. **LLM Agent Processing**  
   - Each screening agent’s (3 total) system prompt is updated with the user-uploaded inex criteria so it knows what it is screening for.  
   - For each study, combine relevant metadata (title, abstract, keywords) into a concise prompt and submit it to the LLM.  
   - The LLM’s output is the decision on whether to include, maybe, or exclude the study based on the inex criteria.  
   - It does this by populating the `decision` column in Supabase with either `include`, `maybe`, or `exclude` along with a rationale.

4. **Output Generation**  
   - Query Supabase to group studies by decision category.  
   - Reassemble the grouped studies into three separate RIS files for **included**, **maybe**, and **excluded**.  
   - Generate an Excel file that maps each study with its decision and a short rationale, ensuring traceability.

5. **User Feedback & Downloads**  
   - Present the user with download options for the three RIS files and the Excel report.  
   - Optionally provide an interface for the user to review “maybe” cases before finalizing the results.

---

## CREW AI OVERVIEW

### Agents

- **Screening Agents (3)** with separate API keys  
- **Reporting Agent**

#### Establish Connection and Query

When an agent starts, it connects to your Supabase database. It queries for studies where the “decision” field is empty (and ideally where no “claim” flag is set).

#### Atomic Claim Attempt

For each unprocessed study, the agent attempts an atomic update—using Supabase’s transactional capabilities—to mark that study as “claimed” (for example, by setting a flag or recording its own identifier).  
- If the update succeeds, the agent “locks” that record for its processing.  
- If it fails (because another agent has claimed it simultaneously), the agent skips that record.

#### Processing the Study

Once a study is claimed, the agent retrieves the full metadata (title, abstract, etc.) and invokes its LLM-based screening logic using the inclusion/exclusion criteria (the system prompt for all the screening agents will be set prior to running).  
- The agent then derives a decision (**include**, **maybe**, or **exclude**) based on the title, abstract, and keywords sent to it, along with a brief rationale.

#### Updating the Record

After processing, the agent updates the corresponding Supabase record with the decision and rationale, and clears the “claim” flag.

#### Rate Limiting and Delay

To prevent overwhelming your LLM API, the agent waits for a predetermined delay (or uses an exponential backoff strategy) before restarting its polling cycle.

#### Looping Continuously

The agent repeats this polling cycle, continuously querying for any new or unclaimed studies until none remain.

This decentralized, atomic-claim approach ensures that each study is processed only once, even if multiple agents are polling simultaneously. It leverages Supabase’s atomic update operations to avoid clashes and CrewAI’s asynchronous execution to manage the load efficiently.

### Reporting Agent

- Once all studies are processed, a reporting agent aggregates the data (counts of included, excluded, and maybe decisions) and generates the final report for the user.  

This design leverages CrewAI’s asynchronous, multi-agent orchestration and its hierarchical process management. It allows you to run multiple LLM tasks in parallel while ensuring that each study is eventually processed and recorded. Of course, you’d also need robust error handling, retries, and monitoring to manage API limits and potential failures—but overall, it’s a sound and practical strategy for your screening tool.

In this setup, every screening agent runs its own loop that checks for studies with an empty `decision` column. When an agent finds an unprocessed study, it first marks that study as “in progress” (using, for example, a status flag or a locking field) to prevent other agents from picking it up simultaneously. Then it processes the study and updates the decision and rationale back to Supabase.

To ensure that you don’t overburden the LLM API, you’d build in rate limiting or delay logic within each screening agent’s loop. This could be as simple as adding a fixed delay between polls or using a more sophisticated asynchronous throttling mechanism. The key is that each agent is aware of its own “busy” state, so it only makes a new API request when it has finished its current task.

To avoid clashes when multiple screening agents poll the database simultaneously, you’d implement a task “claiming” mechanism using atomic updates. Each agent, when polling, would run an update query on the Supabase table that sets a “claimed” or “in-progress” flag (or fills a “claimed_by” field) only if the study’s `decision` field is still empty and it hasn’t already been claimed. This update should be performed as a single atomic transaction so that if two agents try to claim the same study concurrently, only one succeeds.

---

## Supabase

Each row is a study parsed from the RIS file.

- **Column A**: Complete study metadata. This would contain all the raw metadata for each study, possibly stored as a JSON or TEXT field.  
- **Column B**: Title. Extracted from the metadata for quick access and searchability.  
- **Column C**: Abstract. Also extracted for easy access and analysis.  
- **Column D**: Keywords. Could be stored as an array (`TEXT[]` in PostgreSQL) for efficient searching and filtering.  
- **Column E**: Decision. ENUM type with values `'include'`, `'exclude'`, or `'maybe'`.  
- **Column F**: Decision rationale. A TEXT field to store the AI’s reasoning for its decision.

In SQL, this table might look like:

```sql
CREATE TABLE studies (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  metadata JSONB NOT NULL,
  title TEXT,
  abstract TEXT,
  keywords TEXT[],
  decision ENUM('include', 'exclude', 'maybe'),
  decision_rationale TEXT
);

```


**Query Scan**
A good strategy is to limit and “claim” rows in batches using a common table expression (CTE) with row-level locking. 
This minimizes the amount of data scanned per query and avoids collisions between concurrent agents. 
For example:

```sql
WITH to_process AS (
  SELECT id, title, abstract, keywords
  FROM studies
  WHERE decision IS NULL
  FOR UPDATE SKIP LOCKED
  LIMIT 10  -- Adjust batch size as needed
)
UPDATE studies
SET decision = 'processing'  -- or any temporary flag
WHERE id IN (SELECT id FROM to_process)
RETURNING id, title, abstract, keywords;
```
Here’s what happens:

- Selection & Locking: The CTE (to_process) selects a limited batch of rows where the decision column is NULL. The FOR UPDATE SKIP LOCKED clause locks those rows so that if multiple agents run this query concurrently, they won’t pick the same rows.
- Atomic Update: The subsequent UPDATE marks those rows as “processing” (or any state you choose) to signal that they’re claimed.
- Return Data: Finally, the query returns the relevant fields (title, abstract, keywords) for the agent to work with.
- Additionally, make sure you have an index on the decision column so that the WHERE clause runs quickly even as the table grows. This pattern is a standard approach for concurrent queues in PostgreSQL (see discussions on efficient row-limited updates in Supabase, etc.).

Using this design, each of your three AI agents can repeatedly run this query in its own transaction, claim a small batch of unprocessed studies, and then process them without interfering with one another.
